{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18b918eb-19ef-4caa-be8a-2adecc36f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of GPU: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Name: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py:491\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py:523\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_properties\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "#check whether cuda is enabled or not\n",
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d2c78a-42ad-460c-8543-cec758bfadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b4a3d5-9600-4148-9cd0-ae6607e392be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset #65\n",
    "from evaluate import load #55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9934882-fec7-4e78-b0f0-ea8c38af1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4a3e5e-9fea-4a7c-b439-186567f10aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m770310\u001b[0m (\u001b[33m770310-khwopa-engineering-college\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0e7280bb2e4bfc81c8c0b019860ef8f11041f4d4\") # add wandb API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6089f83f-cd88-4749-916c-e9c666e1f123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250302_062315-zm7svsld</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/770310-khwopa-engineering-college/speech/runs/zm7svsld' target=\"_blank\">glowing-capybara-32</a></strong> to <a href='https://wandb.ai/770310-khwopa-engineering-college/speech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/770310-khwopa-engineering-college/speech' target=\"_blank\">https://wandb.ai/770310-khwopa-engineering-college/speech</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/770310-khwopa-engineering-college/speech/runs/zm7svsld' target=\"_blank\">https://wandb.ai/770310-khwopa-engineering-college/speech/runs/zm7svsld</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/770310-khwopa-engineering-college/speech/runs/zm7svsld?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdf555fba60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb11ed8e-39b6-47ca-a288-64a78cfcbf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘csv_files’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘trained_models’: File exists\n",
      "mkdir: cannot create directory ‘checkpoints’: File exists\n",
      "mkdir: cannot create directory ‘tokenizer’: File exists\n",
      "mkdir: cannot create directory ‘processor’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir csv_files\n",
    "!mkdir trained_models\n",
    "!mkdir checkpoints\n",
    "!mkdir tokenizer \n",
    "!mkdir processor \n",
    "!mkdir feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405c1362-3443-465c-8f12-ce6204c80888",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder_path = '/teamspace/studios/this_studio/csv_files'\n",
    "trained_model_path = '/teamspace/studios/this_studio/trained_models/wav2vec0.8dropout'\n",
    "processor_path = '/teamspace/studios/this_studio/processor/wav2vec0.8dropout'\n",
    "checkpoints_path = '/teamspace/studios/this_studio/checkpoints/'\n",
    "tokenizer_path = '/teamspace/studios/this_studio/tokenizer/'\n",
    "feature_extractor_path = '/teamspace/studios/this_studio/feature_extractor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb65ff1-6336-4b20-a62c-50376a702556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/teamspace/studios/this_studio/training_data/a...</td>\n",
       "      <td>कुनै शाखामा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/teamspace/studios/this_studio/training_data/a...</td>\n",
       "      <td>बजे उठेर घरबाट</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/teamspace/studios/this_studio/training_data/a...</td>\n",
       "      <td>तयारीमा समिति लागेको</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/teamspace/studios/this_studio/training_data/a...</td>\n",
       "      <td>रानी हुन्</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/teamspace/studios/this_studio/training_data/a...</td>\n",
       "      <td>जहाँनिया राणाशासनले देशभक्त</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  /teamspace/studios/this_studio/training_data/a...   \n",
       "1  /teamspace/studios/this_studio/training_data/a...   \n",
       "2  /teamspace/studios/this_studio/training_data/a...   \n",
       "3  /teamspace/studios/this_studio/training_data/a...   \n",
       "4  /teamspace/studios/this_studio/training_data/a...   \n",
       "\n",
       "                        labels  \n",
       "0                  कुनै शाखामा  \n",
       "1               बजे उठेर घरबाट  \n",
       "2         तयारीमा समिति लागेको  \n",
       "3                    रानी हुन्  \n",
       "4  जहाँनिया राणाशासनले देशभक्त  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = [\"path\",\"labels\"]\n",
    "audio_path = '/teamspace/studios/this_studio/training_data/audio/'\n",
    "transcript_path = '/teamspace/studios/this_studio/training_data/split_1.csv'\n",
    "df = pd.read_csv(transcript_path,usecols=colnames)\n",
    "df[\"path\"] = audio_path + df[\"path\"] + \".wav\"\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7419be4c-fe00-44ce-a8c7-f31f86a5226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RATIO = 0.90 \n",
    "VAL_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b2f7f70-fc68-4e00-a500-321c095720c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, random_state = 0, train_size = TEST_RATIO)\n",
    "train_df, val_df = train_test_split(train_df, random_state = 0, train_size = VAL_RATIO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facb0e77-2d27-4049-b9d5-628bb455fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(csv_folder_path+'/train.csv',index=False)\n",
    "val_df.to_csv(csv_folder_path+'/val.csv',index=False)\n",
    "test_df.to_csv(csv_folder_path+'/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3fb765-9c0f-4072-a25d-dbb920f03c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"train.csv\",\n",
    "    \"validation\": \"val.csv\",\n",
    "    \"test\": \"test.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "500ca59b-71c6-43c3-ae20-8f09ba843b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe8538d5aa94a949815ad4e2908612b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f96413de3b41ac851ad5daf23f477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a0379ccf32416ca605debdb0e38b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = load_dataset(csv_folder_path, data_files = data_files, split = \"train\")\n",
    "val_data = load_dataset(csv_folder_path, data_files = data_files, split = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f5af9d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/teamspace/studios/this_studio/csv_files/train1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the train, val, and test CSV files from split_1 and split_2\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_split_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/teamspace/studios/this_studio/csv_files/train1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m val_split_1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/csv_files/val1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m test_split_1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/csv_files/test1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/teamspace/studios/this_studio/csv_files/train1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train, val, and test CSV files from split_1 and split_2\n",
    "train_split_1 = pd.read_csv('/teamspace/studios/this_studio/csv_files/train1.csv')\n",
    "val_split_1 = pd.read_csv('/teamspace/studios/this_studio/csv_files/val1.csv')\n",
    "test_split_1 = pd.read_csv('/teamspace/studios/this_studio/csv_files/test1.csv')\n",
    "\n",
    "train_split_2 = pd.read_csv('/teamspace/studios/this_studio/csv_files/train.csv')\n",
    "val_split_2 = pd.read_csv('/teamspace/studios/this_studio/csv_files/val.csv')\n",
    "test_split_2 = pd.read_csv('/teamspace/studios/this_studio/csv_files/test.csv')\n",
    "\n",
    "# Concatenate the corresponding CSV files\n",
    "merged_train_df = pd.concat([train_split_1, train_split_2], ignore_index=True)\n",
    "merged_val_df = pd.concat([val_split_1, val_split_2], ignore_index=True)\n",
    "merged_test_df = pd.concat([test_split_1, test_split_2], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrames to new CSV files\n",
    "merged_train_df.to_csv('/teamspace/studios/this_studio/merged_csvs/merged_train.csv', index=False)\n",
    "merged_val_df.to_csv('/teamspace/studios/this_studio/merged_csvs/merged_val.csv', index=False)\n",
    "merged_test_df.to_csv('/teamspace/studios/this_studio/merged_csvs/merged_test.csv', index=False)\n",
    "\n",
    "# Verify the size of the merged CSV files\n",
    "print(f'Merged train CSV contains {merged_train_df.shape[0]} rows.')\n",
    "print(f'Merged val CSV contains {merged_val_df.shape[0]} rows.')\n",
    "print(f'Merged test CSV contains {merged_test_df.shape[0]} rows.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad78550-8155-4cfb-9f28-5672d6582a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71913a11-a296-406b-8820-723c70ef8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars_to_ignore = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\ ]'\n",
    "chars_to_ignore = r'[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\ ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59232e3-2ccb-45b4-863a-0def78b56c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(batch):\n",
    "    batch[\"labels\"] = re.sub(chars_to_ignore, '', batch[\"labels\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d44e0d-727f-4a8d-918d-fd734716059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80666ce2ae2344a98a88b92288bbe8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bd57f8e7d843bab612f8cd9baff144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(remove_special_characters)\n",
    "val_data = val_data.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c036c239-401c-4061-a8e3-263869f450e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters = {\n",
    "    \"[PAD]\": 0,\n",
    "    \"[UNK]\": 1,\n",
    "    \"|\": 2,\n",
    "    \" \": 3,  # Added space\n",
    "    \"क\": 4,\n",
    "    \"ख\": 5,\n",
    "    \"ग\": 6,\n",
    "    \"घ\": 7,\n",
    "    \"ङ\": 8,\n",
    "    \"च\": 9,\n",
    "    \"छ\": 10,\n",
    "    \"ज\": 11,\n",
    "    \"झ\": 12,\n",
    "    \"ञ\": 13,\n",
    "    \"ट\": 14,\n",
    "    \"ठ\": 15,\n",
    "    \"ड\": 16,\n",
    "    \"ढ\": 17,\n",
    "    \"ण\": 18,\n",
    "    \"त\": 19,\n",
    "    \"थ\": 20,\n",
    "    \"द\": 21,\n",
    "    \"ध\": 22,\n",
    "    \"न\": 23,\n",
    "    \"प\": 24,\n",
    "    \"फ\": 25,\n",
    "    \"ब\": 26,\n",
    "    \"भ\": 27,\n",
    "    \"म\": 28,\n",
    "    \"य\": 29,\n",
    "    \"र\": 30,\n",
    "    \"ल\": 31,\n",
    "    \"व\": 32,\n",
    "    \"श\": 33,\n",
    "    \"ष\": 34,\n",
    "    \"स\": 35,\n",
    "    \"ह\": 36,\n",
    "    \"क्ष\": 37,\n",
    "    \"त्र\": 38,\n",
    "    \"ज्ञ\": 39,\n",
    "    \"अ\": 40,\n",
    "    \"आ\": 41,\n",
    "    \"इ\": 42,\n",
    "    \"ई\": 43,\n",
    "    \"उ\": 44,\n",
    "    \"ऊ\": 45,\n",
    "    \"ए\": 46,\n",
    "    \"ऐ\": 47,\n",
    "    \"ओ\": 48,\n",
    "    \"औ\": 49,\n",
    "    \"अं\": 50,\n",
    "    \"अः\": 51,\n",
    "    \"ा\": 52,\n",
    "    \"ि\": 53,\n",
    "    \"ी\": 54,\n",
    "    \"ु\": 55,\n",
    "    \"ू\": 56,\n",
    "    \"े\": 57,\n",
    "    \"ै\": 58,\n",
    "    \"ो\": 59,\n",
    "    \"ौ\": 60,\n",
    "    \"ं\": 61,\n",
    "    \"ः\": 62,\n",
    "    \"ँ\": 63,\n",
    "    \"०\": 64,\n",
    "    \"१\": 65,\n",
    "    \"२\": 66,\n",
    "    \"३\": 67,\n",
    "    \"४\": 68,\n",
    "    \"५\": 69,\n",
    "    \"६\": 70,\n",
    "    \"७\": 71,\n",
    "    \"८\": 72,\n",
    "    \"९\": 73,\n",
    "    \"्\": 74, # Halant (virama)\n",
    "    \"ऱ्\": 75, \n",
    "    \"ृ\": 76\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f140661e-ef09-49ad-a0b1-d6cdfe7eb096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique letters saved to /teamspace/studios/this_studio/input/cleaned-asr-data/data/vocabulary/vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON format\n",
    "output_file = \"/teamspace/studios/this_studio/input/cleaned-asr-data/data/vocabulary/vocab.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique_letters, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Unique letters saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c83d3448-ef9f-43d3-bf10-b48f2a39b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments, Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, AutoTokenizer\n",
    "\n",
    "# Set environment variable to handle fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Define model and arguments\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0e57bd4-9ebd-4aa2-81f6-f65fb3a30dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = './input/cleaned-asr-data/data/vocabulary/vocab.json'\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_path, \n",
    "    unk_token = \"[UNK]\", \n",
    "    pad_token = \"[PAD]\", \n",
    "    word_delimiter_token = \"|\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0292cfd-2f35-434f-afb8-43f342ced0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/teamspace/studios/this_studio/tokenizer/tokenizer_config.json',\n",
       " '/teamspace/studios/this_studio/tokenizer/special_tokens_map.json',\n",
       " '/teamspace/studios/this_studio/tokenizer/vocab.json',\n",
       " '/teamspace/studios/this_studio/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a61d5e0d-4ced-4170-9c6d-113f0362bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size = 1, \n",
    "    sampling_rate = 16000, \n",
    "    padding_value = 0.0, \n",
    "    do_normalize = True, \n",
    "    return_attention_mask = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1fad9da-bd91-43ea-acb5-12daa885be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor = feature_extractor, \n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5690460-3b11-40d1-bc48-1c23e9665035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(processor_path)\n",
    "# tokenizer= AutoTokenizer.from_pretrained(\"your-model-name\")\n",
    "# tokenizer.save_pretrained(\"path/to/save/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bc3e77c-bcf5-4a36-a3ad-ae5f3cb87f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu124'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7efd1be7-6967-4aef-b820-03ee8aeea46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"labels\"]\n",
    "    \n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
    "    batch[\"sampling_rate\"] = 16000\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c196b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install librosa soundfile torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4fe1b57-8a12-44af-89ac-975a17b7eab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e454e82cb5084b90916cb5e33d0b8609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799bfdc6165847ca98ad170add3393af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(speech_file_to_array_fn, remove_columns=train_data.column_names)\n",
    "val_data = val_data.map(speech_file_to_array_fn, remove_columns=val_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9d3bf68-4875-4d2c-aa3a-2158b9b9d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio  # Import the Audio class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b81c0fa-2493-45d0-81ef-5157c1713ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = random.randint(0, len(train_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e2f6f7c-28f2-49e2-bb1f-8e231c619788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'सिकारखेल्नजान्थे '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_data[rand_int][\"target_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1a81b15-3c9a-4e88-8872-deefdb44283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "        \n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate = batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c924f247-ad58-4824-9905-b221f55fe345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3cb9aacebe4629a14414b24e68d699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb848dcc07f410dad804c0e5150e9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(prepare_dataset, fn_kwargs = {\"processor\": processor}, remove_columns = train_data.column_names, batch_size = 8, num_proc = 4, batched = True)\n",
    "val_data = val_data.map(prepare_dataset, fn_kwargs = {\"processor\": processor}, remove_columns = val_data.column_names, batch_size = 8, num_proc = 4, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b169a4b2-b1db-40d8-864b-83aea93946cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "974836a0-45a3-42fd-9ef6-6cb2ca625c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3943b3ea-8a4f-47a5-afa6-09fb84ebf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor = processor, padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5473c99-3c6b-4f7b-9688-aab7dcbaf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load(\"wer\")\n",
    "cer_metric = load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1394c544-0f27-4e11-b557-374efa165579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Wav2Vec2ForCTC.from_pretrained(\n",
    "#     \"facebook/wav2vec2-large-xlsr-53\", \n",
    "#     attention_dropout = 0.1,\n",
    "#     hidden_dropout = 0.1,\n",
    "#     feat_proj_dropout = 0.0,\n",
    "#     mask_time_prob = 0.05,\n",
    "#     layerdrop = 0.1,\n",
    "#     gradient_checkpointing = True, \n",
    "#     ctc_loss_reduction = \"mean\", \n",
    "#     pad_token_id = processor.tokenizer.pad_token_id,\n",
    "#     vocab_size = len(processor.tokenizer)\n",
    "# )\n",
    "\n",
    "# for checkpoint\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"/teamspace/studios/this_studio/checkpoints/checkpoint-6541\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79fccd9d-dcd8-46cd-a6cf-ab321e719841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a67caf-361b-43a7-b301-e9a204196b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # cer_best = 1 - cer because load best model is considering greater value of cer for better results\n",
    "    cer_best = 1 - cer\n",
    "    # wandb.log({'wer':wer, 'cer':cer})\n",
    "    # experiment.log_metric('wer', wer)\n",
    "    # experiment.log_metric('cer', cer)\n",
    "    \n",
    "    return {\"wer\": wer, \"cer\": cer, \"cer_best\": cer_best}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e4254a3-0b86-444a-a06f-42ac465d598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "# print(\"Transformers version:\", transformers.__version__)\n",
    "# print(\"Accelerate version:\", accelerate.__version__)\n",
    "# print(\"PyTorch version:\", torch.__version__)\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e57ebbe-5069-4646-a7b7-938dc1c74787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02ee4d72-9a72-4f46-b56e-ec2c83493db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/teamspace/studios/this_studio/checkpoints\",  # Path to save training checkpoints\n",
    "    resume_from_checkpoint=True,  # Resume from the latest checkpoint\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=31,\n",
    "    fp16=True,\n",
    "    save_steps=300,\n",
    "    eval_steps=300,\n",
    "    logging_steps=300,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=180,\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be7cd483-a81e-4d2a-acba-71f7873f9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a96f49cd-aeae-4719-8628-230545fceb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the custom MetricsCallback\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.epochs = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Capture metrics during training\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                self.train_loss.append(logs[\"loss\"])\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.val_loss.append(logs[\"eval_loss\"])\n",
    "            if \"epoch\" in logs:\n",
    "                self.epochs.append(logs[\"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "880182e8-9da9-41b3-b7ce-af04c3605414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the callback\n",
    "metrics_callback = MetricsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9742c357-8e78-4661-a1a5-22c621449022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # Your model\n",
    "    data_collator=data_collator,  # Data collator for batching\n",
    "    args=training_args,  # Training arguments\n",
    "    compute_metrics=compute_metrics,  # Function to compute metrics\n",
    "    train_dataset=train_data,  # Training dataset\n",
    "    eval_dataset=val_data,  # Evaluation dataset\n",
    "    # tokenizer=processor.feature_extractor,  # Tokenizer/processor\n",
    "    callbacks=[metrics_callback],  # Add custom callback\n",
    "    processing_class=processor  # Use processing_class\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d83cb8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1450/2984383469.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  trainer.accelerator.scaler = torch.cuda.amp.GradScaler()\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.accelerator.scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50beec38-8a63-49f2-b164-355aa2b277e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='6541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  28/6541 23:29 < 98:03:56, 0.02 it/s, Epoch 0.13/31]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/accelerate/accelerator.py:2325\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:304\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m     device_autocast_ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\n\u001b[1;32m    301\u001b[0m         device_type\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice_autocast_kwargs\n\u001b[1;32m    302\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mis_autocast_available(ctx\u001b[38;5;241m.\u001b[39mdevice_type) \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad(), device_autocast_ctx, torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mcpu_autocast_kwargs):  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdetached_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    307\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (outputs,)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:980\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    978\u001b[0m attn_residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    979\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 980\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    984\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:861\u001b[0m, in \u001b[0;36mWav2Vec2SdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    857\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_causal \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;66;03m# NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80789102-21e5-4f38-9b47-9200fe5550d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a080868-8c02-4543-9132-3d631080dd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 18:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final WER: 80.98%\n",
      "Final CER: 13.94%\n"
     ]
    }
   ],
   "source": [
    "#  Optionally, evaluate final model\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"Final WER: {final_metrics['eval_wer'] * 100:.2f}%\")\n",
    "print(f\"Final CER: {final_metrics['eval_cer'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "780e1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After training, compute final metrics\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe8433d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Final Model Evaluation ====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Added: How to find loss, error, and accuracy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==== Final Model Evaluation ====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal WER (Word Error Rate): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmetrics\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_wer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal CER (Character Error Rate): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_cer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Added: How to find loss, error, and accuracy\n",
    "print(\"\\n==== Final Model Evaluation ====\")\n",
    "print(f\"Final WER (Word Error Rate): {metrics['eval_wer'] * 100:.2f}%\")\n",
    "print(f\"Final CER (Character Error Rate): {metrics['eval_cer'] * 100:.2f}%\")\n",
    "print(f\"Final Validation Loss: {metrics['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e212b6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Added: Calculate Accuracy (1 - WER and 1 - CER)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m word_accuracy \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mmetrics\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_wer\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      3\u001b[0m char_accuracy \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_cer\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord-Level Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Added: Calculate Accuracy (1 - WER and 1 - CER)\n",
    "word_accuracy = (1 - metrics['eval_wer']) * 100\n",
    "char_accuracy = (1 - metrics['eval_cer']) * 100\n",
    "print(f\"Word-Level Accuracy: {word_accuracy:.2f}%\")\n",
    "print(f\"Character-Level Accuracy: {char_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19be9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure loss visualization includes the final evaluation\n",
    "# Prepare data for plotting\n",
    "epochs = metrics_callback.epochs\n",
    "train_loss = metrics_callback.train_loss\n",
    "val_loss = metrics_callback.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39624ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure lengths match for plotting\n",
    "min_length = min(len(epochs), len(train_loss), len(val_loss))\n",
    "epochs = epochs[:min_length]\n",
    "train_loss = train_loss[:min_length]\n",
    "val_loss = val_loss[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "826ed6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation Loss\", marker=\"x\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\", fontsize=16)\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2e506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged split_2 data with existing datasets successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the path for split_2.csv\n",
    "split_2_path = \"/teamspace/studios/this_studio/training_data/split_2.csv\"\n",
    "\n",
    "# Load the data from split_2.csv\n",
    "split_2_df = pd.read_csv(split_2_path)\n",
    "\n",
    "# Split the data from split_2.csv into train, validation, and test sets\n",
    "TEST_RATIO = 0.10  # Test size\n",
    "VAL_RATIO = 0.75   # Validation size (of the train set)\n",
    "train_df, test_df = train_test_split(split_2_df, test_size=TEST_RATIO, random_state=0)\n",
    "train_df, val_df = train_test_split(train_df, test_size=VAL_RATIO, random_state=0)\n",
    "\n",
    "# Save these splits into new CSV files\n",
    "split_2_train_csv = \"/teamspace/studios/this_studio/training_data/split_2_train.csv\"\n",
    "split_2_val_csv = \"/teamspace/studios/this_studio/training_data/split_2_val.csv\"\n",
    "split_2_test_csv = \"/teamspace/studios/this_studio/training_data/split_2_test.csv\"\n",
    "\n",
    "train_df.to_csv(split_2_train_csv, index=False)\n",
    "val_df.to_csv(split_2_val_csv, index=False)\n",
    "test_df.to_csv(split_2_test_csv, index=False)\n",
    "\n",
    "# Now load the existing train.csv, val.csv, and test.csv (from split_1 or previous training)\n",
    "existing_train_path = \"/teamspace/studios/this_studio/csv_files/train.csv\"\n",
    "existing_val_path = \"/teamspace/studios/this_studio/csv_files/val.csv\"\n",
    "existing_test_path = \"/teamspace/studios/this_studio/csv_files/test.csv\"\n",
    "\n",
    "# Read the existing data\n",
    "existing_train_df = pd.read_csv(existing_train_path)\n",
    "existing_val_df = pd.read_csv(existing_val_path)\n",
    "existing_test_df = pd.read_csv(existing_test_path)\n",
    "\n",
    "# Merge the new data from split_2.csv with the existing datasets\n",
    "merged_train_df = pd.concat([existing_train_df, train_df], ignore_index=True)\n",
    "merged_val_df = pd.concat([existing_val_df, val_df], ignore_index=True)\n",
    "merged_test_df = pd.concat([existing_test_df, test_df], ignore_index=True)\n",
    "\n",
    "# Save the merged datasets back to their respective CSV files\n",
    "merged_train_df.to_csv(existing_train_path, index=False)\n",
    "merged_val_df.to_csv(existing_val_path, index=False)\n",
    "merged_test_df.to_csv(existing_test_path, index=False)\n",
    "\n",
    "print(\"✅ Merged split_2 data with existing datasets successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
